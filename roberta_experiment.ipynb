{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pacozabala/CSCI199.X-workbench/blob/main/roberta_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SflUl4hp45Ch",
        "outputId": "c4e3955a-f224-48e2-f976-e8ddfabd95f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'CSCI199.X-workbench'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "git clone https://github.com/Pacozabala/CSCI199.X-workbench"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start running from here if github repo is already cloned."
      ],
      "metadata": {
        "id": "9_IWWyzXr9hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# script to get updates\n",
        "%%bash\n",
        "cd CSCI199.X-workbench\n",
        "git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCdcyK6SiaOJ",
        "outputId": "ed0b02b2-a26b-456b-9807-31beeaea4e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# python -m spacy download en_core_web_md # needed for some spacy functions\n",
        "# pip install transformers datasets scikit-learn"
      ],
      "metadata": {
        "id": "QXpKvHcJe-vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"CSCI199.X-workbench/data/MFRC_polarities.csv\")\n",
        "df = df.dropna(subset=['polarity'])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "u5vRN4-ihjuo",
        "outputId": "3d6b6ea2-fe6d-49e1-86a6-00099873f5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text   subreddit  \\\n",
              "0  That particular part of the debate is especial...      europe   \n",
              "2  TBH Marion Le Pen would be better. Closet fasc...  neoliberal   \n",
              "4  The Le Pen brand of conservatism and classical...      europe   \n",
              "7  Hey, fuck you. Us leftists will never support ...   worldnews   \n",
              "8  Hey, fuck you. Us leftists will never support ...   worldnews   \n",
              "\n",
              "            bucket    annotator        annotation          confidence  \\\n",
              "0  French politics  annotator01            Purity           Confident   \n",
              "2  French politics  annotator02          Fairness  Somewhat Confident   \n",
              "4  French politics  annotator03         Authority  Somewhat Confident   \n",
              "7  French politics  annotator03  Ingroup,Fairness           Confident   \n",
              "8  French politics  annotator04            Purity           Confident   \n",
              "\n",
              "                         polarity  \n",
              "0                   purity.virtue  \n",
              "2                 fairness.virtue  \n",
              "4                authority.virtue  \n",
              "7  ingroup.virtue,fairness.virtue  \n",
              "8                   purity.virtue  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d069e928-08fd-4866-ba12-0b0069453e91\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>bucket</th>\n",
              "      <th>annotator</th>\n",
              "      <th>annotation</th>\n",
              "      <th>confidence</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>That particular part of the debate is especial...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator01</td>\n",
              "      <td>Purity</td>\n",
              "      <td>Confident</td>\n",
              "      <td>purity.virtue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TBH Marion Le Pen would be better. Closet fasc...</td>\n",
              "      <td>neoliberal</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator02</td>\n",
              "      <td>Fairness</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>fairness.virtue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Le Pen brand of conservatism and classical...</td>\n",
              "      <td>europe</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Authority</td>\n",
              "      <td>Somewhat Confident</td>\n",
              "      <td>authority.virtue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Hey, fuck you. Us leftists will never support ...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator03</td>\n",
              "      <td>Ingroup,Fairness</td>\n",
              "      <td>Confident</td>\n",
              "      <td>ingroup.virtue,fairness.virtue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Hey, fuck you. Us leftists will never support ...</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>French politics</td>\n",
              "      <td>annotator04</td>\n",
              "      <td>Purity</td>\n",
              "      <td>Confident</td>\n",
              "      <td>purity.virtue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d069e928-08fd-4866-ba12-0b0069453e91')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d069e928-08fd-4866-ba12-0b0069453e91 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d069e928-08fd-4866-ba12-0b0069453e91');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16639,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9538,\n        \"samples\": [\n          \"My friend reminds me of it constantly and I still feel awful, haha\",\n          \"Thief!\\n\\nYou must return them to their rightful owner, here give them to me, I'll take them there for you I promise\",\n          \"This is definitely just a US and Canada thing. Nobody is standing in Europe (unless they want to) and no one gives a fuck\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subreddit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"relationship_advice\",\n          \"europe\",\n          \"antiwork\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bucket\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"French politics\",\n          \"Everyday Morality\",\n          \"US Politics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotator\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"annotator01\",\n          \"annotator02\",\n          \"annotator05\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"annotation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Ingroup,Fairness,Authority\",\n          \"Fairness,Authority\",\n          \"Purity,Ingroup\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confidence\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Somewhat Confident\",\n          \"Confident\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"polarity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 120,\n        \"samples\": [\n          \"harm.virtue,authority.vice\",\n          \"ingroup.virtue,authority.vice\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"text\", \"polarity\"]].copy()\n",
        "df = df.dropna(subset=[\"polarity\"])\n",
        "\n",
        "df[\"polarity\"] = df[\"polarity\"].str.split(\",\")\n",
        "df = df.explode(\"polarity\")\n",
        "df[\"polarity\"] = df[\"polarity\"].str.strip()\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "\n",
        "all_texts = df[\"text\"].unique()\n",
        "all_labels = sorted(df[\"polarity\"].unique())\n",
        "\n",
        "print(f\"Found {len(all_labels)} labels:\")\n",
        "print(all_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHZ7NHlYi0Lm",
        "outputId": "ca0b8992-375f-47ea-a751-a3654e2301d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 labels:\n",
            "['authority.vice', 'authority.virtue', 'fairness.vice', 'fairness.virtue', 'harm.vice', 'harm.virtue', 'ingroup.vice', 'ingroup.virtue', 'purity.vice', 'purity.virtue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_df = pd.DataFrame({\"text\": all_texts})\n",
        "\n",
        "for label in all_labels:\n",
        "  positive_texts = df[df[\"polarity\"] == label][\"text\"].unique()\n",
        "  multi_df[label] = multi_df[\"text\"].isin(positive_texts).astype(int)"
      ],
      "metadata": {
        "id": "MjRweT9oKKU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33aa1e94",
        "outputId": "dfde5666-e162-4587-8965-e5a676f966e6"
      },
      "source": [
        "virtue_vice_columns = [col for col in multi_df.columns if col.endswith(('.virtue', '.vice'))]\n",
        "\n",
        "for col in virtue_vice_columns:\n",
        "    total_count = multi_df[col].sum()\n",
        "    print(f\"Total count for {col}: {total_count}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total count for authority.vice: 108\n",
            "Total count for authority.virtue: 2797\n",
            "Total count for fairness.vice: 125\n",
            "Total count for fairness.virtue: 4965\n",
            "Total count for harm.vice: 542\n",
            "Total count for harm.virtue: 3991\n",
            "Total count for ingroup.vice: 69\n",
            "Total count for ingroup.virtue: 1771\n",
            "Total count for purity.vice: 116\n",
            "Total count for purity.virtue: 1555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd CSCI199.X-workbench\n",
        "python scripts/prep_binary_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx5TGc3jL9_6",
        "outputId": "a4ce1665-57e9-46ba-ede0-2d514f89a6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file: data/MFRC_polarities.csv\n",
            "Output dir: data/binary_datasets\n",
            "Seed: 42\n",
            "Test size (val+test): 0.2\n",
            "Found 10 labels:\n",
            "['authority.vice', 'authority.virtue', 'fairness.vice', 'fairness.virtue', 'harm.vice', 'harm.virtue', 'ingroup.vice', 'ingroup.virtue', 'purity.vice', 'purity.virtue']\n",
            "Train size: 7630\n",
            "Val size: 954\n",
            "Test size: 954\n",
            "Saved splits for: authority_vice\n",
            "Saved splits for: authority_virtue\n",
            "Saved splits for: fairness_vice\n",
            "Saved splits for: fairness_virtue\n",
            "Saved splits for: harm_vice\n",
            "Saved splits for: harm_virtue\n",
            "Saved splits for: ingroup_vice\n",
            "Saved splits for: ingroup_virtue\n",
            "Saved splits for: purity_vice\n",
            "Saved splits for: purity_virtue\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c370de5c",
        "outputId": "9c5dff58-cdcc-4001-a4c6-048b87464d5c"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "directory_path = 'CSCI199.X-workbench/data/binary_datasets'\n",
        "\n",
        "# Walk through all directories and subdirectories\n",
        "for root, dirs, files in os.walk(directory_path):\n",
        "    for filename in files:\n",
        "        if filename.endswith('train.csv'):\n",
        "            file_path = os.path.join(root, filename)\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "                if 'label' in df.columns:\n",
        "                    label_sum = df['label'].sum()\n",
        "                    print(f\"Sum of 'label' column (from {root}): {label_sum}\")\n",
        "                else:\n",
        "                    print(f\"'{filename}' does not contain a 'label' column.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename} (from {root}): {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/harm_vice): 429\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/purity_virtue): 1233\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/ingroup_virtue): 1427\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/harm_virtue): 3181\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/fairness_vice): 93\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/ingroup_vice): 59\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/authority_vice): 81\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/fairness_virtue): 3969\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/purity_vice): 89\n",
            "Sum of 'label' column (from CSCI199.X-workbench/data/binary_datasets/authority_virtue): 2252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install transformers datasets scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkj4EhI4bPta",
        "outputId": "166abb4a-e97c-4a93-ad54-c652815045ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.21.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: typer>=0.23.1 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.23.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.1->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd CSCI199.X-workbench\n",
        "python scripts/train_roberta.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufeXUpH6sXI9",
        "outputId": "36aa8792-7f9f-471a-d094-9bebdbbb1cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 0.0688\n",
            "Val F1: 0.0000\n",
            "Epoch 2\n",
            "Train Loss: 0.0610\n",
            "Val F1: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "\rLoading weights:   0%|          | 0/197 [00:00<?, ?it/s]\rLoading weights:   1%|          | 1/197 [00:00<00:00, 12905.55it/s, Materializing param=roberta.embeddings.LayerNorm.bias]\rLoading weights:   1%|          | 1/197 [00:00<00:00, 5817.34it/s, Materializing param=roberta.embeddings.LayerNorm.bias] \rLoading weights:   1%|          | 2/197 [00:00<00:00, 4691.62it/s, Materializing param=roberta.embeddings.LayerNorm.weight]\rLoading weights:   1%|          | 2/197 [00:00<00:00, 3903.49it/s, Materializing param=roberta.embeddings.LayerNorm.weight]\rLoading weights:   2%|▏         | 3/197 [00:00<00:00, 4140.48it/s, Materializing param=roberta.embeddings.position_embeddings.weight]\rLoading weights:   2%|▏         | 3/197 [00:00<00:00, 3679.21it/s, Materializing param=roberta.embeddings.position_embeddings.weight]\rLoading weights:   2%|▏         | 4/197 [00:00<00:00, 3356.79it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]\rLoading weights:   2%|▏         | 4/197 [00:00<00:00, 2779.99it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]\rLoading weights:   3%|▎         | 5/197 [00:00<00:00, 3006.24it/s, Materializing param=roberta.embeddings.word_embeddings.weight]      \rLoading weights:   3%|▎         | 5/197 [00:00<00:00, 2808.56it/s, Materializing param=roberta.embeddings.word_embeddings.weight]\rLoading weights:   3%|▎         | 6/197 [00:00<00:00, 2678.93it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]\rLoading weights:   3%|▎         | 6/197 [00:00<00:00, 2537.90it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]\rLoading weights:   4%|▎         | 7/197 [00:00<00:00, 2527.99it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]\rLoading weights:   4%|▎         | 7/197 [00:00<00:00, 2317.30it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]\rLoading weights:   4%|▍         | 8/197 [00:00<00:00, 2361.49it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]      \rLoading weights:   4%|▍         | 8/197 [00:00<00:00, 2195.11it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]\rLoading weights:   5%|▍         | 9/197 [00:00<00:00, 2309.64it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]\rLoading weights:   5%|▍         | 9/197 [00:00<00:00, 2171.46it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]\rLoading weights:   5%|▌         | 10/197 [00:00<00:00, 2193.44it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias]     \rLoading weights:   5%|▌         | 10/197 [00:00<00:00, 2065.65it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias]\rLoading weights:   6%|▌         | 11/197 [00:00<00:00, 2113.77it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight]\rLoading weights:   6%|▌         | 11/197 [00:00<00:00, 2021.62it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight]\rLoading weights:   6%|▌         | 12/197 [00:00<00:00, 2062.86it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias]\rLoading weights:   6%|▌         | 12/197 [00:00<00:00, 2029.09it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias]\rLoading weights:   7%|▋         | 13/197 [00:00<00:00, 2021.88it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight]\rLoading weights:   7%|▋         | 13/197 [00:00<00:00, 1949.65it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight]\rLoading weights:   7%|▋         | 14/197 [00:00<00:00, 1982.65it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias]  \rLoading weights:   7%|▋         | 14/197 [00:00<00:00, 1912.65it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias]\rLoading weights:   8%|▊         | 15/197 [00:00<00:00, 1982.19it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight]\rLoading weights:   8%|▊         | 15/197 [00:00<00:00, 1916.37it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight]\rLoading weights:   8%|▊         | 16/197 [00:00<00:00, 1930.91it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias]    \rLoading weights:   8%|▊         | 16/197 [00:00<00:00, 1902.99it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias]\rLoading weights:   9%|▊         | 17/197 [00:00<00:00, 1920.68it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight]\rLoading weights:   9%|▊         | 17/197 [00:00<00:00, 1894.65it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight]\rLoading weights:   9%|▉         | 18/197 [00:00<00:00, 1927.78it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]    \rLoading weights:   9%|▉         | 18/197 [00:00<00:00, 1878.98it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]\rLoading weights:  10%|▉         | 19/197 [00:00<00:00, 1905.95it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]\rLoading weights:  10%|▉         | 19/197 [00:00<00:00, 1860.48it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]\rLoading weights:  10%|█         | 20/197 [00:00<00:00, 1897.88it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]      \rLoading weights:  10%|█         | 20/197 [00:00<00:00, 1874.05it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]\rLoading weights:  11%|█         | 21/197 [00:00<00:00, 1891.56it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]\rLoading weights:  11%|█         | 21/197 [00:00<00:00, 1869.87it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]\rLoading weights:  11%|█         | 22/197 [00:00<00:00, 1897.64it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]\rLoading weights:  11%|█         | 22/197 [00:00<00:00, 1855.25it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]\rLoading weights:  12%|█▏        | 23/197 [00:00<00:00, 1869.48it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]\rLoading weights:  12%|█▏        | 23/197 [00:00<00:00, 1850.87it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]\rLoading weights:  12%|█▏        | 24/197 [00:00<00:00, 1864.41it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]      \rLoading weights:  12%|█▏        | 24/197 [00:00<00:00, 1846.66it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]\rLoading weights:  13%|█▎        | 25/197 [00:00<00:00, 1888.27it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]\rLoading weights:  13%|█▎        | 25/197 [00:00<00:00, 1855.03it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]\rLoading weights:  13%|█▎        | 26/197 [00:00<00:00, 1892.67it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]      \rLoading weights:  13%|█▎        | 26/197 [00:00<00:00, 1880.40it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]\rLoading weights:  14%|█▎        | 27/197 [00:00<00:00, 1923.34it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]\rLoading weights:  14%|█▎        | 27/197 [00:00<00:00, 1911.68it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]\rLoading weights:  14%|█▍        | 28/197 [00:00<00:00, 1929.52it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]\rLoading weights:  14%|█▍        | 28/197 [00:00<00:00, 1917.18it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]\rLoading weights:  15%|█▍        | 29/197 [00:00<00:00, 1953.82it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]\rLoading weights:  15%|█▍        | 29/197 [00:00<00:00, 1941.40it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]\rLoading weights:  15%|█▌        | 30/197 [00:00<00:00, 1979.01it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]  \rLoading weights:  15%|█▌        | 30/197 [00:00<00:00, 1967.74it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]\rLoading weights:  16%|█▌        | 31/197 [00:00<00:00, 1979.98it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]\rLoading weights:  16%|█▌        | 31/197 [00:00<00:00, 1967.93it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]\rLoading weights:  16%|█▌        | 32/197 [00:00<00:00, 2001.07it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]    \rLoading weights:  16%|█▌        | 32/197 [00:00<00:00, 1989.86it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]\rLoading weights:  17%|█▋        | 33/197 [00:00<00:00, 2023.63it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]\rLoading weights:  17%|█▋        | 33/197 [00:00<00:00, 2012.86it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]\rLoading weights:  17%|█▋        | 34/197 [00:00<00:00, 2034.24it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]    \rLoading weights:  17%|█▋        | 34/197 [00:00<00:00, 2023.10it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]\rLoading weights:  18%|█▊        | 35/197 [00:00<00:00, 2035.79it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]\rLoading weights:  18%|█▊        | 35/197 [00:00<00:00, 2024.84it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]\rLoading weights:  18%|█▊        | 36/197 [00:00<00:00, 2034.18it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]      \rLoading weights:  18%|█▊        | 36/197 [00:00<00:00, 2023.44it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]\rLoading weights:  19%|█▉        | 37/197 [00:00<00:00, 2054.51it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]\rLoading weights:  19%|█▉        | 37/197 [00:00<00:00, 2044.71it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]\rLoading weights:  19%|█▉        | 38/197 [00:00<00:00, 2076.23it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]\rLoading weights:  19%|█▉        | 38/197 [00:00<00:00, 2066.43it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]\rLoading weights:  20%|█▉        | 39/197 [00:00<00:00, 2074.52it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]\rLoading weights:  20%|█▉        | 39/197 [00:00<00:00, 2063.55it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]\rLoading weights:  20%|██        | 40/197 [00:00<00:00, 2089.84it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]      \rLoading weights:  20%|██        | 40/197 [00:00<00:00, 2080.10it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]\rLoading weights:  21%|██        | 41/197 [00:00<00:00, 2096.26it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]\rLoading weights:  21%|██        | 41/197 [00:00<00:00, 2068.84it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]\rLoading weights:  21%|██▏       | 42/197 [00:00<00:00, 2096.25it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]      \rLoading weights:  21%|██▏       | 42/197 [00:00<00:00, 2087.04it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]\rLoading weights:  22%|██▏       | 43/197 [00:00<00:00, 2113.45it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]\rLoading weights:  22%|██▏       | 43/197 [00:00<00:00, 2104.40it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]\rLoading weights:  22%|██▏       | 44/197 [00:00<00:00, 2108.77it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]\rLoading weights:  22%|██▏       | 44/197 [00:00<00:00, 2095.72it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]\rLoading weights:  23%|██▎       | 45/197 [00:00<00:00, 2120.12it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]\rLoading weights:  23%|██▎       | 45/197 [00:00<00:00, 2111.30it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]\rLoading weights:  23%|██▎       | 46/197 [00:00<00:00, 2137.84it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]  \rLoading weights:  23%|██▎       | 46/197 [00:00<00:00, 2129.42it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]\rLoading weights:  24%|██▍       | 47/197 [00:00<00:00, 2134.71it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]\rLoading weights:  24%|██▍       | 47/197 [00:00<00:00, 2122.60it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]\rLoading weights:  24%|██▍       | 48/197 [00:00<00:00, 2125.69it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]    \rLoading weights:  24%|██▍       | 48/197 [00:00<00:00, 2109.86it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]\rLoading weights:  25%|██▍       | 49/197 [00:00<00:00, 2133.97it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]\rLoading weights:  25%|██▍       | 49/197 [00:00<00:00, 2125.89it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]\rLoading weights:  25%|██▌       | 50/197 [00:00<00:00, 2139.86it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]    \rLoading weights:  25%|██▌       | 50/197 [00:00<00:00, 2131.51it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]\rLoading weights:  26%|██▌       | 51/197 [00:00<00:00, 2145.34it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]\rLoading weights:  26%|██▌       | 51/197 [00:00<00:00, 2137.00it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]\rLoading weights:  26%|██▋       | 52/197 [00:00<00:00, 2154.45it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]      \rLoading weights:  26%|██▋       | 52/197 [00:00<00:00, 2146.31it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]\rLoading weights:  27%|██▋       | 53/197 [00:00<00:00, 2158.09it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]\rLoading weights:  27%|██▋       | 53/197 [00:00<00:00, 2132.27it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]\rLoading weights:  27%|██▋       | 54/197 [00:00<00:00, 2153.28it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]\rLoading weights:  27%|██▋       | 54/197 [00:00<00:00, 2145.55it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]\rLoading weights:  28%|██▊       | 55/197 [00:00<00:00, 2166.11it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]\rLoading weights:  28%|██▊       | 55/197 [00:00<00:00, 2158.69it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]\rLoading weights:  28%|██▊       | 56/197 [00:00<00:00, 2162.71it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]      \rLoading weights:  28%|██▊       | 56/197 [00:00<00:00, 2154.58it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]\rLoading weights:  29%|██▉       | 57/197 [00:00<00:00, 2160.08it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]\rLoading weights:  29%|██▉       | 57/197 [00:00<00:00, 2152.57it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]\rLoading weights:  29%|██▉       | 58/197 [00:00<00:00, 2170.15it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]      \rLoading weights:  29%|██▉       | 58/197 [00:00<00:00, 2162.63it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]\rLoading weights:  30%|██▉       | 59/197 [00:00<00:00, 2147.08it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]\rLoading weights:  30%|██▉       | 59/197 [00:00<00:00, 2139.65it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]\rLoading weights:  30%|███       | 60/197 [00:00<00:00, 2140.19it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]\rLoading weights:  30%|███       | 60/197 [00:00<00:00, 2132.82it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]\rLoading weights:  31%|███       | 61/197 [00:00<00:00, 2129.83it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]\rLoading weights:  31%|███       | 61/197 [00:00<00:00, 2122.39it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]\rLoading weights:  31%|███▏      | 62/197 [00:00<00:00, 2128.77it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]  \rLoading weights:  31%|███▏      | 62/197 [00:00<00:00, 2120.82it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]\rLoading weights:  32%|███▏      | 63/197 [00:00<00:00, 2139.69it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]\rLoading weights:  32%|███▏      | 63/197 [00:00<00:00, 2133.56it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]\rLoading weights:  32%|███▏      | 64/197 [00:00<00:00, 2128.24it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]    \rLoading weights:  32%|███▏      | 64/197 [00:00<00:00, 2121.33it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]\rLoading weights:  33%|███▎      | 65/197 [00:00<00:00, 2138.73it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]\rLoading weights:  33%|███▎      | 65/197 [00:00<00:00, 2132.64it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]\rLoading weights:  34%|███▎      | 66/197 [00:00<00:00, 2124.17it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]    \rLoading weights:  34%|███▎      | 66/197 [00:00<00:00, 2116.93it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]\rLoading weights:  34%|███▍      | 67/197 [00:00<00:00, 2120.85it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]\rLoading weights:  34%|███▍      | 67/197 [00:00<00:00, 2111.06it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]\rLoading weights:  35%|███▍      | 68/197 [00:00<00:00, 2113.39it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]      \rLoading weights:  35%|███▍      | 68/197 [00:00<00:00, 2105.56it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]\rLoading weights:  35%|███▌      | 69/197 [00:00<00:00, 2086.82it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]\rLoading weights:  35%|███▌      | 69/197 [00:00<00:00, 2078.64it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]\rLoading weights:  36%|███▌      | 70/197 [00:00<00:00, 2088.14it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]\rLoading weights:  36%|███▌      | 70/197 [00:00<00:00, 2073.10it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]\rLoading weights:  36%|███▌      | 71/197 [00:00<00:00, 2075.66it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]\rLoading weights:  36%|███▌      | 71/197 [00:00<00:00, 2069.88it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]\rLoading weights:  37%|███▋      | 72/197 [00:00<00:00, 2077.56it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]      \rLoading weights:  37%|███▋      | 72/197 [00:00<00:00, 2063.07it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]\rLoading weights:  37%|███▋      | 73/197 [00:00<00:00, 2071.57it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]\rLoading weights:  37%|███▋      | 73/197 [00:00<00:00, 2064.63it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]\rLoading weights:  38%|███▊      | 74/197 [00:00<00:00, 2066.20it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]      \rLoading weights:  38%|███▊      | 74/197 [00:00<00:00, 2060.44it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]\rLoading weights:  38%|███▊      | 75/197 [00:00<00:00, 2076.05it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]\rLoading weights:  38%|███▊      | 75/197 [00:00<00:00, 2071.07it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]\rLoading weights:  39%|███▊      | 76/197 [00:00<00:00, 2073.04it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]\rLoading weights:  39%|███▊      | 76/197 [00:00<00:00, 2067.73it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]\rLoading weights:  39%|███▉      | 77/197 [00:00<00:00, 2070.84it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]\rLoading weights:  39%|███▉      | 77/197 [00:00<00:00, 2065.46it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]\rLoading weights:  40%|███▉      | 78/197 [00:00<00:00, 2072.17it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]  \rLoading weights:  40%|███▉      | 78/197 [00:00<00:00, 2058.92it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]\rLoading weights:  40%|████      | 79/197 [00:00<00:00, 2066.92it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]\rLoading weights:  40%|████      | 79/197 [00:00<00:00, 2060.57it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]\rLoading weights:  41%|████      | 80/197 [00:00<00:00, 2056.61it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]    \rLoading weights:  41%|████      | 80/197 [00:00<00:00, 2051.53it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]\rLoading weights:  41%|████      | 81/197 [00:00<00:00, 2065.46it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]\rLoading weights:  41%|████      | 81/197 [00:00<00:00, 2060.86it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]\rLoading weights:  42%|████▏     | 82/197 [00:00<00:00, 2068.50it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]    \rLoading weights:  42%|████▏     | 82/197 [00:00<00:00, 2063.66it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]\rLoading weights:  42%|████▏     | 83/197 [00:00<00:00, 2066.29it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]\rLoading weights:  42%|████▏     | 83/197 [00:00<00:00, 2061.38it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]\rLoading weights:  43%|████▎     | 84/197 [00:00<00:00, 2069.75it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]      \rLoading weights:  43%|████▎     | 84/197 [00:00<00:00, 2056.72it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]\rLoading weights:  43%|████▎     | 85/197 [00:00<00:00, 2062.32it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]\rLoading weights:  43%|████▎     | 85/197 [00:00<00:00, 2050.21it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]\rLoading weights:  44%|████▎     | 86/197 [00:00<00:00, 2053.22it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]\rLoading weights:  44%|████▎     | 86/197 [00:00<00:00, 2047.23it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]\rLoading weights:  44%|████▍     | 87/197 [00:00<00:00, 2051.04it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]\rLoading weights:  44%|████▍     | 87/197 [00:00<00:00, 2046.36it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]\rLoading weights:  45%|████▍     | 88/197 [00:00<00:00, 2058.52it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]      \rLoading weights:  45%|████▍     | 88/197 [00:00<00:00, 2047.11it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]\rLoading weights:  45%|████▌     | 89/197 [00:00<00:00, 2058.61it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]\rLoading weights:  45%|████▌     | 89/197 [00:00<00:00, 2054.36it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]\rLoading weights:  46%|████▌     | 90/197 [00:00<00:00, 2061.69it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]      \rLoading weights:  46%|████▌     | 90/197 [00:00<00:00, 2057.34it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]\rLoading weights:  46%|████▌     | 91/197 [00:00<00:00, 2065.54it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]\rLoading weights:  46%|████▌     | 91/197 [00:00<00:00, 2061.28it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]\rLoading weights:  47%|████▋     | 92/197 [00:00<00:00, 2067.42it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]\rLoading weights:  47%|████▋     | 92/197 [00:00<00:00, 2063.06it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]\rLoading weights:  47%|████▋     | 93/197 [00:00<00:00, 2075.31it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]\rLoading weights:  47%|████▋     | 93/197 [00:00<00:00, 2071.44it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]\rLoading weights:  48%|████▊     | 94/197 [00:00<00:00, 2075.13it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]  \rLoading weights:  48%|████▊     | 94/197 [00:00<00:00, 2069.62it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]\rLoading weights:  48%|████▊     | 95/197 [00:00<00:00, 2071.02it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]\rLoading weights:  48%|████▊     | 95/197 [00:00<00:00, 2065.52it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]\rLoading weights:  49%|████▊     | 96/197 [00:00<00:00, 2071.08it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]    \rLoading weights:  49%|████▊     | 96/197 [00:00<00:00, 2060.56it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]\rLoading weights:  49%|████▉     | 97/197 [00:00<00:00, 2065.89it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]\rLoading weights:  49%|████▉     | 97/197 [00:00<00:00, 2060.67it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]\rLoading weights:  50%|████▉     | 98/197 [00:00<00:00, 2063.32it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]    \rLoading weights:  50%|████▉     | 98/197 [00:00<00:00, 2057.62it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]\rLoading weights:  50%|█████     | 99/197 [00:00<00:00, 2063.26it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]\rLoading weights:  50%|█████     | 99/197 [00:00<00:00, 2052.95it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]\rLoading weights:  51%|█████     | 100/197 [00:00<00:00, 2058.67it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]     \rLoading weights:  51%|█████     | 100/197 [00:00<00:00, 2041.88it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]\rLoading weights:  51%|█████▏    | 101/197 [00:00<00:00, 2044.98it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]\rLoading weights:  51%|█████▏    | 101/197 [00:00<00:00, 2040.01it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]\rLoading weights:  52%|█████▏    | 102/197 [00:00<00:00, 2042.39it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.bias]\rLoading weights:  52%|█████▏    | 102/197 [00:00<00:00, 2037.47it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.bias]\rLoading weights:  52%|█████▏    | 103/197 [00:00<00:00, 2042.39it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.weight]\rLoading weights:  52%|█████▏    | 103/197 [00:00<00:00, 2033.07it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.weight]\rLoading weights:  53%|█████▎    | 104/197 [00:00<00:00, 2039.26it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.bias]      \rLoading weights:  53%|█████▎    | 104/197 [00:00<00:00, 2034.44it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.bias]\rLoading weights:  53%|█████▎    | 105/197 [00:00<00:00, 2036.68it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.weight]\rLoading weights:  53%|█████▎    | 105/197 [00:00<00:00, 2025.94it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.weight]\rLoading weights:  54%|█████▍    | 106/197 [00:00<00:00, 2036.72it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.bias]      \rLoading weights:  54%|█████▍    | 106/197 [00:00<00:00, 2033.22it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.bias]\rLoading weights:  54%|█████▍    | 107/197 [00:00<00:00, 2031.88it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.weight]\rLoading weights:  54%|█████▍    | 107/197 [00:00<00:00, 2028.18it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.weight]\rLoading weights:  55%|█████▍    | 108/197 [00:00<00:00, 2028.87it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.bias]\rLoading weights:  55%|█████▍    | 108/197 [00:00<00:00, 2025.18it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.bias]\rLoading weights:  55%|█████▌    | 109/197 [00:00<00:00, 2031.08it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.weight]\rLoading weights:  55%|█████▌    | 109/197 [00:00<00:00, 2013.85it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.weight]\rLoading weights:  56%|█████▌    | 110/197 [00:00<00:00, 2019.52it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.bias]  \rLoading weights:  56%|█████▌    | 110/197 [00:00<00:00, 2015.06it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.bias]\rLoading weights:  56%|█████▋    | 111/197 [00:00<00:00, 2016.57it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.weight]\rLoading weights:  56%|█████▋    | 111/197 [00:00<00:00, 2011.85it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.weight]\rLoading weights:  57%|█████▋    | 112/197 [00:00<00:00, 2015.36it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.bias]    \rLoading weights:  57%|█████▋    | 112/197 [00:00<00:00, 2004.88it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.bias]\rLoading weights:  57%|█████▋    | 113/197 [00:00<00:00, 2013.53it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.weight]\rLoading weights:  57%|█████▋    | 113/197 [00:00<00:00, 2010.31it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.weight]\rLoading weights:  58%|█████▊    | 114/197 [00:00<00:00, 2007.55it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.bias]    \rLoading weights:  58%|█████▊    | 114/197 [00:00<00:00, 2004.10it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.bias]\rLoading weights:  58%|█████▊    | 115/197 [00:00<00:00, 2011.20it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.weight]\rLoading weights:  58%|█████▊    | 115/197 [00:00<00:00, 2007.90it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.weight]\rLoading weights:  59%|█████▉    | 116/197 [00:00<00:00, 2010.77it/s, Materializing param=roberta.encoder.layer.6.output.dense.bias]      \rLoading weights:  59%|█████▉    | 116/197 [00:00<00:00, 2006.61it/s, Materializing param=roberta.encoder.layer.6.output.dense.bias]\rLoading weights:  59%|█████▉    | 117/197 [00:00<00:00, 2011.33it/s, Materializing param=roberta.encoder.layer.6.output.dense.weight]\rLoading weights:  59%|█████▉    | 117/197 [00:00<00:00, 2002.89it/s, Materializing param=roberta.encoder.layer.6.output.dense.weight]\rLoading weights:  60%|█████▉    | 118/197 [00:00<00:00, 2011.27it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.bias]\rLoading weights:  60%|█████▉    | 118/197 [00:00<00:00, 2008.13it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.bias]\rLoading weights:  60%|██████    | 119/197 [00:00<00:00, 2013.94it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.weight]\rLoading weights:  60%|██████    | 119/197 [00:00<00:00, 2010.72it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.weight]\rLoading weights:  61%|██████    | 120/197 [00:00<00:00, 2015.06it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.bias]      \rLoading weights:  61%|██████    | 120/197 [00:00<00:00, 2006.48it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.bias]\rLoading weights:  61%|██████▏   | 121/197 [00:00<00:00, 2007.19it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.weight]\rLoading weights:  61%|██████▏   | 121/197 [00:00<00:00, 2004.09it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.weight]\rLoading weights:  62%|██████▏   | 122/197 [00:00<00:00, 2009.38it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.bias]      \rLoading weights:  62%|██████▏   | 122/197 [00:00<00:00, 2006.34it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.bias]\rLoading weights:  62%|██████▏   | 123/197 [00:00<00:00, 2015.55it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.weight]\rLoading weights:  62%|██████▏   | 123/197 [00:00<00:00, 2012.77it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.weight]\rLoading weights:  63%|██████▎   | 124/197 [00:00<00:00, 2019.26it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.bias]\rLoading weights:  63%|██████▎   | 124/197 [00:00<00:00, 2016.69it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.bias]\rLoading weights:  63%|██████▎   | 125/197 [00:00<00:00, 2026.63it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.weight]\rLoading weights:  63%|██████▎   | 125/197 [00:00<00:00, 2024.26it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.weight]\rLoading weights:  64%|██████▍   | 126/197 [00:00<00:00, 2034.79it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.bias]  \rLoading weights:  64%|██████▍   | 126/197 [00:00<00:00, 2032.53it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.bias]\rLoading weights:  64%|██████▍   | 127/197 [00:00<00:00, 2043.23it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.weight]\rLoading weights:  64%|██████▍   | 127/197 [00:00<00:00, 2041.09it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.weight]\rLoading weights:  65%|██████▍   | 128/197 [00:00<00:00, 2051.76it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.bias]    \rLoading weights:  65%|██████▍   | 128/197 [00:00<00:00, 2049.58it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.bias]\rLoading weights:  65%|██████▌   | 129/197 [00:00<00:00, 2059.81it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.weight]\rLoading weights:  65%|██████▌   | 129/197 [00:00<00:00, 2056.93it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.weight]\rLoading weights:  66%|██████▌   | 130/197 [00:00<00:00, 2067.36it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.bias]    \rLoading weights:  66%|██████▌   | 130/197 [00:00<00:00, 2065.01it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.bias]\rLoading weights:  66%|██████▋   | 131/197 [00:00<00:00, 2075.53it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.weight]\rLoading weights:  66%|██████▋   | 131/197 [00:00<00:00, 2073.37it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.weight]\rLoading weights:  67%|██████▋   | 132/197 [00:00<00:00, 2083.95it/s, Materializing param=roberta.encoder.layer.7.output.dense.bias]      \rLoading weights:  67%|██████▋   | 132/197 [00:00<00:00, 2081.86it/s, Materializing param=roberta.encoder.layer.7.output.dense.bias]\rLoading weights:  68%|██████▊   | 133/197 [00:00<00:00, 2092.47it/s, Materializing param=roberta.encoder.layer.7.output.dense.weight]\rLoading weights:  68%|██████▊   | 133/197 [00:00<00:00, 2089.91it/s, Materializing param=roberta.encoder.layer.7.output.dense.weight]\rLoading weights:  68%|██████▊   | 134/197 [00:00<00:00, 2100.13it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.bias]\rLoading weights:  68%|██████▊   | 134/197 [00:00<00:00, 2097.96it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.bias]\rLoading weights:  69%|██████▊   | 135/197 [00:00<00:00, 2107.74it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.weight]\rLoading weights:  69%|██████▊   | 135/197 [00:00<00:00, 2105.56it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.weight]\rLoading weights:  69%|██████▉   | 136/197 [00:00<00:00, 2115.77it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.bias]      \rLoading weights:  69%|██████▉   | 136/197 [00:00<00:00, 2113.67it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.bias]\rLoading weights:  70%|██████▉   | 137/197 [00:00<00:00, 2123.94it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.weight]\rLoading weights:  70%|██████▉   | 137/197 [00:00<00:00, 2121.87it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.weight]\rLoading weights:  70%|███████   | 138/197 [00:00<00:00, 2131.66it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.bias]      \rLoading weights:  70%|███████   | 138/197 [00:00<00:00, 2129.53it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.bias]\rLoading weights:  71%|███████   | 139/197 [00:00<00:00, 2139.43it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.weight]\rLoading weights:  71%|███████   | 139/197 [00:00<00:00, 2137.33it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.weight]\rLoading weights:  71%|███████   | 140/197 [00:00<00:00, 2147.46it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.bias]\rLoading weights:  71%|███████   | 140/197 [00:00<00:00, 2145.42it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.bias]\rLoading weights:  72%|███████▏  | 141/197 [00:00<00:00, 2155.44it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.weight]\rLoading weights:  72%|███████▏  | 141/197 [00:00<00:00, 2153.38it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.weight]\rLoading weights:  72%|███████▏  | 142/197 [00:00<00:00, 2162.80it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.bias]  \rLoading weights:  72%|███████▏  | 142/197 [00:00<00:00, 2160.48it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.bias]\rLoading weights:  73%|███████▎  | 143/197 [00:00<00:00, 2170.12it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.weight]\rLoading weights:  73%|███████▎  | 143/197 [00:00<00:00, 2168.03it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.weight]\rLoading weights:  73%|███████▎  | 144/197 [00:00<00:00, 2177.88it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.bias]    \rLoading weights:  73%|███████▎  | 144/197 [00:00<00:00, 2175.74it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.bias]\rLoading weights:  74%|███████▎  | 145/197 [00:00<00:00, 2185.52it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.weight]\rLoading weights:  74%|███████▎  | 145/197 [00:00<00:00, 2183.42it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.weight]\rLoading weights:  74%|███████▍  | 146/197 [00:00<00:00, 2193.21it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.bias]    \rLoading weights:  74%|███████▍  | 146/197 [00:00<00:00, 2190.56it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.bias]\rLoading weights:  75%|███████▍  | 147/197 [00:00<00:00, 2200.02it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.weight]\rLoading weights:  75%|███████▍  | 147/197 [00:00<00:00, 2197.87it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.weight]\rLoading weights:  75%|███████▌  | 148/197 [00:00<00:00, 2206.78it/s, Materializing param=roberta.encoder.layer.8.output.dense.bias]      \rLoading weights:  75%|███████▌  | 148/197 [00:00<00:00, 2204.69it/s, Materializing param=roberta.encoder.layer.8.output.dense.bias]\rLoading weights:  76%|███████▌  | 149/197 [00:00<00:00, 2214.36it/s, Materializing param=roberta.encoder.layer.8.output.dense.weight]\rLoading weights:  76%|███████▌  | 149/197 [00:00<00:00, 2212.26it/s, Materializing param=roberta.encoder.layer.8.output.dense.weight]\rLoading weights:  76%|███████▌  | 150/197 [00:00<00:00, 2221.94it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.bias]\rLoading weights:  76%|███████▌  | 150/197 [00:00<00:00, 2219.80it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.bias]\rLoading weights:  77%|███████▋  | 151/197 [00:00<00:00, 2228.63it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.weight]\rLoading weights:  77%|███████▋  | 151/197 [00:00<00:00, 2226.48it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.weight]\rLoading weights:  77%|███████▋  | 152/197 [00:00<00:00, 2235.37it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.bias]      \rLoading weights:  77%|███████▋  | 152/197 [00:00<00:00, 2233.17it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.bias]\rLoading weights:  78%|███████▊  | 153/197 [00:00<00:00, 2242.36it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.weight]\rLoading weights:  78%|███████▊  | 153/197 [00:00<00:00, 2240.24it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.weight]\rLoading weights:  78%|███████▊  | 154/197 [00:00<00:00, 2249.61it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.bias]      \rLoading weights:  78%|███████▊  | 154/197 [00:00<00:00, 2247.53it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.bias]\rLoading weights:  79%|███████▊  | 155/197 [00:00<00:00, 2256.29it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.weight]\rLoading weights:  79%|███████▊  | 155/197 [00:00<00:00, 2254.11it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.weight]\rLoading weights:  79%|███████▉  | 156/197 [00:00<00:00, 2263.11it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.bias]\rLoading weights:  79%|███████▉  | 156/197 [00:00<00:00, 2260.90it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.bias]\rLoading weights:  80%|███████▉  | 157/197 [00:00<00:00, 2270.09it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.weight]\rLoading weights:  80%|███████▉  | 157/197 [00:00<00:00, 2267.99it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.weight]\rLoading weights:  80%|████████  | 158/197 [00:00<00:00, 2277.16it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.bias]  \rLoading weights:  80%|████████  | 158/197 [00:00<00:00, 2275.07it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.bias]\rLoading weights:  81%|████████  | 159/197 [00:00<00:00, 2284.19it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.weight]\rLoading weights:  81%|████████  | 159/197 [00:00<00:00, 2280.86it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.weight]\rLoading weights:  81%|████████  | 160/197 [00:00<00:00, 2289.80it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.bias]    \rLoading weights:  81%|████████  | 160/197 [00:00<00:00, 2287.46it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.bias]\rLoading weights:  82%|████████▏ | 161/197 [00:00<00:00, 2296.44it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.weight]\rLoading weights:  82%|████████▏ | 161/197 [00:00<00:00, 2294.30it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.weight]\rLoading weights:  82%|████████▏ | 162/197 [00:00<00:00, 2303.29it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.bias]    \rLoading weights:  82%|████████▏ | 162/197 [00:00<00:00, 2301.24it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.bias]\rLoading weights:  83%|████████▎ | 163/197 [00:00<00:00, 2310.25it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.weight]\rLoading weights:  83%|████████▎ | 163/197 [00:00<00:00, 2307.83it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.weight]\rLoading weights:  83%|████████▎ | 164/197 [00:00<00:00, 2316.65it/s, Materializing param=roberta.encoder.layer.9.output.dense.bias]      \rLoading weights:  83%|████████▎ | 164/197 [00:00<00:00, 2314.60it/s, Materializing param=roberta.encoder.layer.9.output.dense.bias]\rLoading weights:  84%|████████▍ | 165/197 [00:00<00:00, 2323.25it/s, Materializing param=roberta.encoder.layer.9.output.dense.weight]\rLoading weights:  84%|████████▍ | 165/197 [00:00<00:00, 2321.16it/s, Materializing param=roberta.encoder.layer.9.output.dense.weight]\rLoading weights:  84%|████████▍ | 166/197 [00:00<00:00, 2330.08it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.bias]\rLoading weights:  84%|████████▍ | 166/197 [00:00<00:00, 2327.96it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.bias]\rLoading weights:  85%|████████▍ | 167/197 [00:00<00:00, 2336.62it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.weight]\rLoading weights:  85%|████████▍ | 167/197 [00:00<00:00, 2334.47it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.weight]\rLoading weights:  85%|████████▌ | 168/197 [00:00<00:00, 2342.68it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.bias]      \rLoading weights:  85%|████████▌ | 168/197 [00:00<00:00, 2340.49it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.bias]\rLoading weights:  86%|████████▌ | 169/197 [00:00<00:00, 2348.85it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.weight]\rLoading weights:  86%|████████▌ | 169/197 [00:00<00:00, 2346.72it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.weight]\rLoading weights:  86%|████████▋ | 170/197 [00:00<00:00, 2355.34it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.bias]      \rLoading weights:  86%|████████▋ | 170/197 [00:00<00:00, 2353.26it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.bias]\rLoading weights:  87%|████████▋ | 171/197 [00:00<00:00, 2361.92it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.weight]\rLoading weights:  87%|████████▋ | 171/197 [00:00<00:00, 2359.85it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.weight]\rLoading weights:  87%|████████▋ | 172/197 [00:00<00:00, 2367.94it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.bias]\rLoading weights:  87%|████████▋ | 172/197 [00:00<00:00, 2365.23it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.bias]\rLoading weights:  88%|████████▊ | 173/197 [00:00<00:00, 2373.41it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.weight]\rLoading weights:  88%|████████▊ | 173/197 [00:00<00:00, 2371.26it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.weight]\rLoading weights:  88%|████████▊ | 174/197 [00:00<00:00, 2379.53it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.bias]  \rLoading weights:  88%|████████▊ | 174/197 [00:00<00:00, 2377.35it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.bias]\rLoading weights:  89%|████████▉ | 175/197 [00:00<00:00, 2385.71it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.weight]\rLoading weights:  89%|████████▉ | 175/197 [00:00<00:00, 2383.68it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.weight]\rLoading weights:  89%|████████▉ | 176/197 [00:00<00:00, 2392.12it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.bias]    \rLoading weights:  89%|████████▉ | 176/197 [00:00<00:00, 2389.39it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.bias]\rLoading weights:  90%|████████▉ | 177/197 [00:00<00:00, 2397.47it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.weight]\rLoading weights:  90%|████████▉ | 177/197 [00:00<00:00, 2395.21it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.weight]\rLoading weights:  90%|█████████ | 178/197 [00:00<00:00, 2403.39it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.bias]    \rLoading weights:  90%|█████████ | 178/197 [00:00<00:00, 2401.24it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.bias]\rLoading weights:  91%|█████████ | 179/197 [00:00<00:00, 2409.46it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.weight]\rLoading weights:  91%|█████████ | 179/197 [00:00<00:00, 2407.36it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.weight]\rLoading weights:  91%|█████████▏| 180/197 [00:00<00:00, 2415.66it/s, Materializing param=roberta.encoder.layer.10.output.dense.bias]      \rLoading weights:  91%|█████████▏| 180/197 [00:00<00:00, 2413.64it/s, Materializing param=roberta.encoder.layer.10.output.dense.bias]\rLoading weights:  92%|█████████▏| 181/197 [00:00<00:00, 2421.51it/s, Materializing param=roberta.encoder.layer.10.output.dense.weight]\rLoading weights:  92%|█████████▏| 181/197 [00:00<00:00, 2419.44it/s, Materializing param=roberta.encoder.layer.10.output.dense.weight]\rLoading weights:  92%|█████████▏| 182/197 [00:00<00:00, 2427.52it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.bias]\rLoading weights:  92%|█████████▏| 182/197 [00:00<00:00, 2425.43it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.bias]\rLoading weights:  93%|█████████▎| 183/197 [00:00<00:00, 2433.48it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.weight]\rLoading weights:  93%|█████████▎| 183/197 [00:00<00:00, 2431.31it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.weight]\rLoading weights:  93%|█████████▎| 184/197 [00:00<00:00, 2439.36it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.bias]      \rLoading weights:  93%|█████████▎| 184/197 [00:00<00:00, 2437.28it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.bias]\rLoading weights:  94%|█████████▍| 185/197 [00:00<00:00, 2444.76it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.weight]\rLoading weights:  94%|█████████▍| 185/197 [00:00<00:00, 2442.60it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.weight]\rLoading weights:  94%|█████████▍| 186/197 [00:00<00:00, 2450.39it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.bias]      \rLoading weights:  94%|█████████▍| 186/197 [00:00<00:00, 2448.31it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.bias]\rLoading weights:  95%|█████████▍| 187/197 [00:00<00:00, 2456.28it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.weight]\rLoading weights:  95%|█████████▍| 187/197 [00:00<00:00, 2454.20it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.weight]\rLoading weights:  95%|█████████▌| 188/197 [00:00<00:00, 2462.18it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.bias]\rLoading weights:  95%|█████████▌| 188/197 [00:00<00:00, 2460.18it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.bias]\rLoading weights:  96%|█████████▌| 189/197 [00:00<00:00, 2468.21it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.weight]\rLoading weights:  96%|█████████▌| 189/197 [00:00<00:00, 2465.58it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.weight]\rLoading weights:  96%|█████████▋| 190/197 [00:00<00:00, 2473.19it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.bias]  \rLoading weights:  96%|█████████▋| 190/197 [00:00<00:00, 2470.93it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.bias]\rLoading weights:  97%|█████████▋| 191/197 [00:00<00:00, 2478.59it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.weight]\rLoading weights:  97%|█████████▋| 191/197 [00:00<00:00, 2476.49it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.weight]\rLoading weights:  97%|█████████▋| 192/197 [00:00<00:00, 2484.23it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.bias]    \rLoading weights:  97%|█████████▋| 192/197 [00:00<00:00, 2482.16it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.bias]\rLoading weights:  98%|█████████▊| 193/197 [00:00<00:00, 2489.97it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.weight]\rLoading weights:  98%|█████████▊| 193/197 [00:00<00:00, 2487.99it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.weight]\rLoading weights:  98%|█████████▊| 194/197 [00:00<00:00, 2495.35it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.bias]    \rLoading weights:  98%|█████████▊| 194/197 [00:00<00:00, 2493.30it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.bias]\rLoading weights:  99%|█████████▉| 195/197 [00:00<00:00, 2500.77it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.weight]\rLoading weights:  99%|█████████▉| 195/197 [00:00<00:00, 2498.74it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.weight]\rLoading weights:  99%|█████████▉| 196/197 [00:00<00:00, 2506.49it/s, Materializing param=roberta.encoder.layer.11.output.dense.bias]      \rLoading weights:  99%|█████████▉| 196/197 [00:00<00:00, 2504.44it/s, Materializing param=roberta.encoder.layer.11.output.dense.bias]\rLoading weights: 100%|██████████| 197/197 [00:00<00:00, 2512.20it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]\rLoading weights: 100%|██████████| 197/197 [00:00<00:00, 2510.21it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]\rLoading weights: 100%|██████████| 197/197 [00:00<00:00, 2503.82it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]\n",
            "RobertaForSequenceClassification LOAD REPORT from: roberta-base\n",
            "Key                             | Status     | \n",
            "--------------------------------+------------+-\n",
            "lm_head.layer_norm.weight       | UNEXPECTED | \n",
            "lm_head.bias                    | UNEXPECTED | \n",
            "lm_head.dense.bias              | UNEXPECTED | \n",
            "roberta.embeddings.position_ids | UNEXPECTED | \n",
            "lm_head.layer_norm.bias         | UNEXPECTED | \n",
            "lm_head.dense.weight            | UNEXPECTED | \n",
            "classifier.dense.weight         | MISSING    | \n",
            "classifier.out_proj.bias        | MISSING    | \n",
            "classifier.dense.bias           | MISSING    | \n",
            "classifier.out_proj.weight      | MISSING    | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMi8hnfqhzhaqseeL1QXOFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}